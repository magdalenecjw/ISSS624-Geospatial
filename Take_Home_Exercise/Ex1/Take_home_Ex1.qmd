---
title: "1: Geospatial Analytics for Public Good"
author: "Magdalene Chan"
date: 2023-11-25
date-modified: "last-modified"
execute: 
  warning: false
format:
  html:
    code-fold: true
---

# Objectives

As city-wide urban infrastructures become increasingly digital, datasets from technologies like GPS and RFID on vehicles offer opportunities to track movement patterns over space and time. For instance, smart cards and GPS devices on public buses collect routes and ridership data, containing valuable structure and patterns for understanding human movement and behavior within cities. Despite their potential, the practical use of these extensive location-aware datasets often remains limited to basic tracking and mapping within GIS applications due to the lack of comprehensive spatial and spatio-temporal analysis functions in conventional GIS tools.

Exploratory Spatial Data Analysis (ESDA) holds tremendous potential to address such complex problems. In this study, appropriate Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) will be applied to undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

# Tasks

The following tasks will be undertaken in this exercise:

## Geovisualisation and Analysis

1.  Compute the passenger trips generated by origin at a hexagon level of 250m for the following time periods:
    1.  Weekday morning peak -- 6am to 9am (inclusive)
    2.  Weekday evening peak -- 5pm to 8pm (inclusive)
    3.  Weekend/holiday morning peak -- 11am to 2pm (inclusive)
    4.  Weekend/holiday evening peak -- 4pm to 7pm (inclusive)
2.  Display the geographical distribution of the passenger trips using appropriate geovisualisation methods.
3.  Describe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).

## Emerging Hot Spot Analysis

With reference to the passenger trips generated by origin at the hexagon level for the four time periods given above:

1.  Perform Mann-Kendall Test by using the spatio-temporal local G~i~\* values,
2.  Prepare EHSA maps of the G~i~\* values of the passenger trips by origin at the hexagon level. The maps should only display the significant (i.e. p-value \< 0.05).
3.  With reference to the EHSA maps and data visualisation prepared, describe the spatial patterns revealed (not more than 250 words per cluster).

# Getting Started

The code chunk below uses `p_load()` of `pacman` package to check if the required packages have been installed on the computer. If they are, the packages will be launched. The following packages will be used:

-   `sf` package is used for importing, managing, and processing geospatial data.
-   `sfdep` package is used to create spatial weights matrix and LISA objects using the sf class to represent spatial data.
-   `tmap` package is used for thematic mapping.
-   `plotly` package is used to create interactive graphs and charts.
-   `tidyverse` package is used for aspatial data wrangling.
-   `knitr` package is used for dynamic report generation in R.

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse, knitr)
```

# Import Data

The data sets used are:

-   Bus Stop Location (Last updated Jul 2023) from [LTADataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html) retrieved on 18 Nov 2023
-   Passenger Volume by Origin Destination Bus Stops for August 2023 from [LTADataMall](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html) retrieved on 18 Nov 2023

## Import Geospatial Data: Bus Stop Locations

The code chunk below uses the `st_read()` function of **sf** package to import `BusStop` shapefile into R as a simple feature data frame called `BusStop`. As `BusStop` uses **svy21** projected coordinate system, the `crs` is set to 3414.

```{r}
BusStop <- st_read(dsn = "data/geospatial", 
                layer = "BusStop") %>%
  st_transform(crs=3414)

# Examine the structure of the data frame
str(BusStop)
```

There are a total of 5161 features in the `BusStop` shapefile. Notably, `BUS_STOP_N` is listed as a character variable. As this variable will be used as the identifier to link to the aspatial data, it should be transformed to a factor so that R treats it as a grouping variable.

```{r}
# Apply as.factor() to the column
BusStop$BUS_STOP_N <- as.factor(BusStop$BUS_STOP_N)

# Re-examine the structure of the data frame
str(BusStop)
```

Based on the output above, `BUS_STOP_N` is now a factor of 5145 levels. However, there are a total of 5161 observations. As such, the duplicate records should be checked. The code chunk below shows a list of duplicated bus stop codes.

```{r}
duplicate <- BusStop %>%
  group_by(BUS_STOP_N) %>%
  filter(n() > 1) %>%
  arrange(BUS_STOP_N)

kable(duplicate)
```

The code chunk below uses the `distinct()` function to keep only the unique rows based on the `BUS_STOP_N` while preserving all other fields (`.keep_all = TRUE`). By default, `distinct()` keeps the first occurrence of each unique combination of values in the specified columns.

```{r}
BusStop <- BusStop %>%
  distinct(BUS_STOP_N, .keep_all = TRUE)

# Re-examine the structure of the data frame
str(BusStop)
```

There are now a total of 5145 observations, aligned with the number of factor levels.

## Import Passenger Volume by Origin-Destination Bus Stops

The code chunk below uses the `read_csv()` function of `readr` package (imported with the `tidyverse` package) to import the csv files into R.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202308.csv")

# Examine the structure of the data frame
str(odbus)
```

Based on the data frame structure seen above, `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` are listed as character variables. These variables are equivalent to `BUS_STOP_N` of `BusStop` sf data frame and should be transformed to factors so that R treats them as grouping variables.

```{r}
# Columns to convert to factors
columns_to_convert <- c("ORIGIN_PT_CODE", "DESTINATION_PT_CODE")

# Apply as.factor() to the adjusted columns
odbus[columns_to_convert] <- lapply(odbus[columns_to_convert], as.factor)

# Re-examine the structure of the data frame
str(odbus)
```

Based on the output above, `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` are now factors.

## Extract Commuting Flow data

The code chunk below extracts commuting flows for the four target time periods. For completeness, a list of all possible Bus Stops x Day Time x Time combinations is created. Following which, the commuting flow for each Bus Stops x Day Time x Time combination within the `odbus` data frame is computed and missing combinations are imputed with a value of 0. Lastly the resultant data frame is pivoted wider to form a data frame where each row represents one bus stop code, similar to the BusStop sf data frame.

```{r}
# Create a list of possible bus stop code, day type and time per hour for checking
BusStopList <- expand.grid(ORIGIN_PT_CODE = unique(BusStop$BUS_STOP_N),
                          DAY_TYPE = c("WEEKDAY", "WEEKENDS/HOLIDAY"),
                          TIME_PER_HOUR = unique(odbus$TIME_PER_HOUR))

# Commute commuting flow by bus stop code, day type and time per hour
odbus_recoded <- odbus %>%
  group_by(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) 

# Identify missing Bus Stops and impute a 0 value
odbus_missing <- BusStopList %>%
  anti_join(odbus_recoded, by = c("ORIGIN_PT_CODE", "DAY_TYPE", "TIME_PER_HOUR")) %>%
  mutate(TRIPS = 0)

# Combine the above two data frames
odbus_long <- rbind(odbus_recoded, odbus_missing) %>%
    mutate(peak_period = case_when(
    DAY_TYPE == "WEEKDAY" & TIME_PER_HOUR >= 6 & TIME_PER_HOUR <=9 ~ 
      "weekday_morn",
    DAY_TYPE == "WEEKDAY" & TIME_PER_HOUR >= 17 & TIME_PER_HOUR <=20 ~ 
      "weekday_evening",
    DAY_TYPE == "WEEKENDS/HOLIDAY" & TIME_PER_HOUR >= 11 & TIME_PER_HOUR <=14 ~ 
      "weekend_morn",
    DAY_TYPE == "WEEKENDS/HOLIDAY" & TIME_PER_HOUR >= 16 & TIME_PER_HOUR <=19 ~ 
      "weekend_evening",
    TRUE ~ "non_peak")) 

# Pivot wider to form data frame with one bus stop code per row
odbus_wide_peak <- odbus_long %>%
  group_by(ORIGIN_PT_CODE, peak_period) %>%
  summarise(TOTAL_TRIPS = sum(TRIPS)) %>%
  pivot_wider(names_from = peak_period,
              values_from = TOTAL_TRIPS)

odbus_wide <- odbus_long %>%
  filter(!(peak_period == "non_peak")) %>%
  group_by(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR) %>%
  summarise(TOTAL_TRIPS = sum(TRIPS)) %>%
  mutate(Day_Time = paste(DAY_TYPE, TIME_PER_HOUR)) %>%
  ungroup %>%
  select(-c(DAY_TYPE, TIME_PER_HOUR)) %>%
  pivot_wider(names_from = Day_Time,
              values_from = TOTAL_TRIPS) %>%
  left_join(odbus_wide_peak)
```

## Append Commuting Flow data with Bus Stop Locations

`left_join()` of **dplyr** is then used to join the geographical data and commuting flow data table using the Bus Stop Code as the common identifier. Left join is done to ensure that the geospatial properties (geometry column) of the `BusStop` sf data frame is retained and bus stops not found within the geospatial data set are dropped.

```{r}
odBusStop <- left_join(BusStop, odbus_wide, by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))

str(odBusStop)
```

There are a total of 5145 observations, which matches the number of features in the `BusStop` sf data frame.

# Geovisualisation and Analysis

## Create Spatial Grids

Spatial grids are commonly used in spatial analysis to divide the study area into equal-sized, regular polygons that can tessellate the whole study area. After that, a variable is summarized within each polygon. For this exercise, a hexagon layer of 250m (where 250m is the perpendicular distance between the hexagon centre and its edges) will be created.

In the code chunk below, `st_make_grid` is used to create the hexagon grids using the `svy21` projected coordinate system. In `st_make_grid`, `cellsize` argument refers to the cell size in the units that the crs of the spatial data is using. The cellsize can be defined as the distance between opposite edges. Since the data is set in SVY21 projected coordinate system, which [uses metres as the unit](https://epsg.io/3414), the value is set as `c(500,500)` to create a hexagon layer of 250m. The resulting data frame is then converted into a sf data frame and an index is added for each hexagon grid.

```{r}
#| eval: false
hex_grid <- st_make_grid(odBusStop, cellsize = c(500, 500), 
                         crs = 3413, what = "polygons", square = FALSE) %>%
  st_sf() %>%
  mutate(index = row_number())
```

In the code chunk below, `st_join` and `st_intersects()` is used to return a list of bus stops lying within each hexagon grid. A left join is done in this process (using the argument `left = TRUE`) to ensure that the geospatial properties (geometry column) of the `hex_grid` sf data frame is retained. After filtering out hexagon grids that do not contain any bus stops, the number of bus stops and the total number of trips at each time period is then computed for each hexagon grid.

```{r}
#| eval: false
hex_grid_count <- st_join(hex_grid, odBusStop, join = st_intersects, left = TRUE) %>%
  filter(!(is.na(BUS_STOP_N))) %>%
  group_by(index) %>%
  summarise(
    count = n(),
    bus_stop_codes = paste(BUS_STOP_N, collapse = ", "),
    bus_stop_names = paste(LOC_DESC, collapse = ", "),
    across(starts_with("week"), sum)
  ) %>%
  ungroup()
```

The `hex_grid_count` sf data frame can now be used for plotting the geographical distribution of peak hour bus stops and commuting flow using **tmap**.

```{r}
#| echo: false
#| eval: false
# Save the output as rds files instead of rendering the above code chunks
write_rds(hex_grid, "data/rds/hex_grid.rds")
write_rds(hex_grid_count, "data/rds/hex_grid_count.rds")
```

```{r}
#| echo: false
# Load the rds files instead of rendering the above code chunks
hex_grid <- read_rds("data/rds/hex_grid.rds")
hex_grid_count <- read_rds("data/rds/hex_grid_count.rds")
```

## Visualisation of Geographical Distribution of Commuting Flow

### Continuous Scale

For efficiency of plotting, select just the columns that will be used for this section.

```{r}
hex_grid_count2 <- hex_grid_count %>%
  select(index, count, bus_stop_codes, bus_stop_names, geometry,
         weekday_evening, weekday_morn, weekend_evening, weekend_morn)
```

The following code chunks will review the geographical distribution of the weekdays and weekends peak hour commuting flows using a continuous scale fill.

::: panel-tabset
### Geographical Distribution of Weekday Peak Hour Commuting Flows

The geographical distribution of the weekdays peak hour commuting flows is plotted side by side using `tm_facets()` function. The `free.scales` argument is set to `FALSE` to show a consistent scale across both time periods.

```{r}
tmap_mode("view")

tm_shape(hex_grid_count2) +
  tm_fill(col = c("weekday_morn", "weekday_evening"),
          palette = "Blues",
          style = "cont", 
          popup.vars = c("Bus Stops" = "bus_stop_codes", 
                         "Weekday Morning" = "weekday_morn", 
                         "Weekday Evening" = "weekday_evening")) + 
  tm_layout(title = c("Weekday Morning", "Weekday Evening")) +
  tm_facets(ncol = 1, free.scales = FALSE)
```

::: callout-tip
## Insights

The visualisation above shows a heavily right-skewed distribution across all the weekday peak hour time periods. Most bus stops have less than 100k commuter trips at each of these time periods. Notable hexagon grids with significantly more commuting flow \>300k are:

-   Weekday Mornings: 1251, 2411
-   Weekday Evenings: 1251, 2411, 3239, 4349, 4539
:::

The code chunk below displays the hexagon grids with significantly more commuting flow \>300k in greater detail.

```{r}
weekday_300k <- hex_grid_count2 %>%
  filter(weekday_morn >= 300000 | weekday_evening >= 300000) %>%
  select(index, bus_stop_names, weekday_morn, weekday_evening)

kable(weekday_300k)
```

### Geographical Distribution of Weekend/PH Peak Hour Commuting Flows

The geographical distribution of the weekends / holidays peak hour commuting flows is plotted side by side using `tm_facets()` function. The `free.scales` argument is set to `FALSE` to show a consistent scale across both time periods.

```{r}
tmap_mode("view")

tm_shape(hex_grid_count2) +
  tm_fill(col = c("weekend_morn", "weekend_evening"),
          palette = "Purples",
          style = "cont", 
          popup.vars = c("Bus Stops" = "bus_stop_codes", 
                         "Weekend/PH Morning" = "weekend_morn", 
                         "Weekend/PH Evening" = "weekend_evening")) + 
  tm_layout(title = c("Weekend/PH Morning", "Weekend/PH Evening")) +
  tm_facets(ncol = 1, free.scales = FALSE)
```

::: callout-tip
## Insights

Similarly, the visualisation above shows a heavily right-skewed distribution across all the weekend peak hour time periods. However, the continuous scale suggests that weekend peak hour commuting flow volume is approximately one-fifth of weekday peak hour commuting flow volume. Most bus stops have less than 20k commuter trips at each of these time periods. Notable hexagon grids with significantly more commuting flow \>80k are:

-   Weekend/PH Mornings: 1251, 2411, 4349, 4539
-   Weekend/PH Evenings: 1251, 2411, 3239, 3578, 4349, 4539
:::

The code chunk below displays the hexagon grids with significantly more commuting flow \>80k in greater detail.

```{r}
weekend_80k <- hex_grid_count2 %>%
  filter(weekend_morn >= 80000 | weekend_evening >= 80000) %>%
  select(index, bus_stop_names, weekend_morn, weekend_evening)

kable(weekend_80k)
```
:::

However, due to the heavily right-skewed distribution, not much can be glimpsed from the current geovisualisation using continuous scale fill. A data classification method will be used to replot the maps.

### Jenks Natural Breaks

The Jenks optimization method, also called the Jenks natural breaks classification method, is a data clustering method designed to determine the best arrangement of values into different classes by seeking to reduce the variance within classes and maximize the variance between classes. For this exercise, the number of breaks is defined at `n=6`.

The following code chunks will review the geographical distribution of the weekdays and weekends peak hour commuting flows using a continuous scale fill.

::: panel-tabset
### Geographical Distribution of Weekday Peak Hour Commuting Flows

The geographical distribution of the weekdays peak hour commuting flows is plotted side by side using `tm_facets()` function. The `free.scales` argument is set to `FALSE` to show a consistent scale across both time periods.

```{r}
tmap_mode("view")

tm_shape(hex_grid_count2) +
  tm_fill(col = c("weekday_morn", "weekday_evening"),
          palette = "Blues",
          n = 6, style = "jenks", 
          popup.vars = c("Bus Stops" = "bus_stop_codes", 
                         "Weekday Morning" = "weekday_morn", 
                         "Weekday Evening" = "weekday_evening")) + 
  tm_layout(title = c("Weekday Morning", "Weekday Evening")) +
  tm_facets(ncol = 1, free.scales = FALSE)
```

The code chunk below displays the hexagon grids that fall into the last two classes (\>156k) in greater detail.

```{r}
weekday_156k <- hex_grid_count2 %>%
  filter(weekday_morn >= 156000 | weekday_evening >= 156000) %>%
  select(index, bus_stop_names, weekday_morn, weekday_evening)

kable(weekday_156k)
```

::: callout-tip
## Insights

Using Jenks Natural Breaks to classify the weekday peak hour commuting flow reveals more potential hot spots in the housing estates. This will be verified in the second part of this study on Emerging Hot Spots Analysis.
:::

### Geographical Distribution of Weekend/PH Peak Hour Commuting Flows

The geographical distribution of the weekends / holidays peak hour commuting flows is plotted side by side using `tm_facets()` function. The `free.scales` argument is set to `FALSE` to show a consistent scale across both time periods.

```{r}
tmap_mode("view")

tm_shape(hex_grid_count2) +
  tm_fill(col = c("weekend_morn", "weekend_evening"),
          palette = "Purples",
          n = 6, style = "jenks", 
          popup.vars = c("Bus Stops" = "bus_stop_codes", 
                         "Weekend/PH Morning" = "weekend_morn", 
                         "Weekend/PH Evening" = "weekend_evening")) + 
  tm_layout(title = c("Weekend/PH Morning", "Weekend/PH Evening")) +
  tm_facets(ncol = 1, free.scales = FALSE)
```

The code chunk below displays the hexagon grids that fall into the last two classes (\>31k) in greater detail.

```{r}
weekend_31k <- hex_grid_count2 %>%
  filter(weekend_morn >= 31000 | weekend_evening >= 31000) %>%
  select(index, bus_stop_names, weekend_morn, weekend_evening)

kable(weekend_31k)
```

::: callout-tip
## Insights

Similarly, using Jenks Natural Breaks to classify the weekend peak hour commuting flow reveals more potential hot spots not just in the major housing estates, but also in the town area and the area surrounding Woodlands Checkpoint. This will be verified in the second part of this study on Emerging Hot Spots Analysis.
:::
:::

# Emerging Hot Spot Analysis

The aim of Emerging Hot Spot Analysis (EHSA) is to [classify each location into one of seventeen categories based on ESRI's emerging hot spot classification criteria](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html). To perform EHSA, time series cubes are needed.

## Time Series Cube

Spatio-temporal data are typically presented in long formats, where a row identifies a unique location and time observation represented by a column dedicated to time and another to locations. However, such data are traditionally not linked to the geographies as they contain only an identifier of the location, but not the spatial representation that makes the data meaningful for geospatial analysis.

The `spacetime` class in **sfdep** package can be used to create lattice data that is a representation of spatio-temporal data for a set of regions containing the geometries over a number of different time-periods. There are two possible layouts: spatio-temporal full grid and sparse grids.

Given a number of spatial features `n`, and time periods `m`, a **spatio-temporal full grid** contains `n×m` rows. Each location has a recorded observation for each of the time periods in `m`. When there are missing observations for some locations or time periods and they are entirely omitted from the data set, that is a **spatio-temporal sparse grid**.

For this analysis, time series cubes will be created for each of the four target time periods respectively using the earlier created `hex_grid_count`.

## Create Time Series Cube

In the code chunks below, relevant columns that correspond to the target time periods will be selected from `hex_grid_count` and joined with `hex_grid` to create the `n` number of spatial features. The corresponding sf data frame will then be pivoted longer to create a time series data containing the `m` time periods. Once the data frame has `nxm` rows, apply the `spacetime()` function to create the time series cube.

::: panel-tabset
### Weekday Morning Time Series Cube

```{r}
# Select relevant columns from hex_grid_count
weekday_morn_hex <- hex_grid_count %>%
  select(index, count, bus_stop_codes, bus_stop_names, geometry,
         `WEEKDAY 6`, `WEEKDAY 7`, `WEEKDAY 8`, `WEEKDAY 9`)

# Join with hex_grid and pivot longer to create time series data
weekday_morn_full <- st_join(hex_grid, weekday_morn_hex, join = st_within) %>%
  select(-c(index.y)) %>%
  rename(index = index.x) %>%
  rename_with(~gsub("WEEKDAY ", "", .), matches("^WEEKDAY \\d+$")) %>%
  pivot_longer(cols = c(`6`, `7`, `8`, `9`), 
               names_to = "TIME_PER_HOUR", values_to = "TRIPS",
               names_transform = as.integer, values_transform = as.integer) %>%
  mutate(across(where(is.integer), ~ifelse(is.na(.), 0, .))) %>%
  select(index, TIME_PER_HOUR, TRIPS) %>%
  st_set_geometry(NULL)

# Apply spacetime() function
weekday_morn_cube <- spacetime(weekday_morn_full, hex_grid,
                               .loc_col = "index", .time_col = "TIME_PER_HOUR")

# Verify that resultant output is a spacetime cube
is_spacetime_cube(weekday_morn_cube)
```

The `TRUE` return confirms that `weekday_morn_cube` object is indeed an time-space cube.

### Weekday Evening Time Series Cube

```{r}
# Select relevant columns from hex_grid_count
weekday_evening_hex <- hex_grid_count %>%
  select(index, count, bus_stop_codes, bus_stop_names, geometry,
         `WEEKDAY 17`, `WEEKDAY 18`, `WEEKDAY 19`, `WEEKDAY 20`)

# Join with hex_grid and pivot longer to create time series data
weekday_evening_full <- st_join(hex_grid, weekday_evening_hex, join = st_within) %>%
  select(-c(index.y)) %>%
  rename(index = index.x) %>%
  rename_with(~gsub("WEEKDAY ", "", .), matches("^WEEKDAY \\d+$")) %>%
  pivot_longer(cols = c(`17`, `18`, `19`, `20`), 
               names_to = "TIME_PER_HOUR", values_to = "TRIPS",
               names_transform = as.integer, values_transform = as.integer) %>%
  mutate(across(where(is.integer), ~ifelse(is.na(.), 0, .))) %>%
  select(index, TIME_PER_HOUR, TRIPS) %>%
  st_set_geometry(NULL)

# Apply spacetime() function
weekday_evening_cube <- spacetime(weekday_evening_full, hex_grid,
                               .loc_col = "index", .time_col = "TIME_PER_HOUR")

# Verify that resultant output is a spacetime cube
is_spacetime_cube(weekday_evening_cube)
```

The `TRUE` return confirms that `weekday_evening_cube` object is indeed an time-space cube.

### Weekend/PH Morning Time Series Cube

```{r}
# Select relevant columns from hex_grid_count
weekend_morn_hex <- hex_grid_count %>%
  select(index, count, bus_stop_codes, bus_stop_names, geometry,
         `WEEKENDS/HOLIDAY 11`, `WEEKENDS/HOLIDAY 12`, 
         `WEEKENDS/HOLIDAY 13`, `WEEKENDS/HOLIDAY 14`)

# Join with hex_grid and pivot longer to create time series data
weekend_morn_full <- st_join(hex_grid, weekend_morn_hex, join = st_within) %>%
  select(-c(index.y)) %>%
  rename(index = index.x) %>%
  rename_with(~gsub("WEEKENDS/HOLIDAY ", "", .), matches("^WEEKENDS/HOLIDAY \\d+$")) %>%
  pivot_longer(cols = c(`11`, `12`, `13`, `14`), 
               names_to = "TIME_PER_HOUR", values_to = "TRIPS",
               names_transform = as.integer, values_transform = as.integer) %>%
  mutate(across(where(is.integer), ~ifelse(is.na(.), 0, .))) %>%
  select(index, TIME_PER_HOUR, TRIPS) %>%
  st_set_geometry(NULL)

# Apply spacetime() function
weekend_morn_cube <- spacetime(weekend_morn_full, hex_grid,
                               .loc_col = "index", .time_col = "TIME_PER_HOUR")

# Verify that resultant output is a spacetime cube
is_spacetime_cube(weekend_morn_cube)
```

The `TRUE` return confirms that `weekend_morn_cube` object is indeed an time-space cube.

### Weekend/PH Evening Time Series Cube

```{r}
# Select relevant columns from hex_grid_count
weekend_evening_hex <- hex_grid_count %>%
  select(index, count, bus_stop_codes, bus_stop_names, geometry,
         `WEEKENDS/HOLIDAY 16`, `WEEKENDS/HOLIDAY 17`, 
         `WEEKENDS/HOLIDAY 18`, `WEEKENDS/HOLIDAY 19`)

# Join with hex_grid and pivot longer to create time series data
weekend_evening_full <- st_join(hex_grid, weekend_evening_hex, join = st_within) %>%
  select(-c(index.y)) %>%
  rename(index = index.x) %>%
  rename_with(~gsub("WEEKENDS/HOLIDAY ", "", .), matches("^WEEKENDS/HOLIDAY \\d+$")) %>%
  pivot_longer(cols = c(`16`, `17`, `18`, `19`), 
               names_to = "TIME_PER_HOUR", values_to = "TRIPS",
               names_transform = as.integer, values_transform = as.integer) %>%
  mutate(across(where(is.integer), ~ifelse(is.na(.), 0, .))) %>%
  select(index, TIME_PER_HOUR, TRIPS) %>%
  st_set_geometry(NULL)

# Apply spacetime() function
weekend_evening_cube <- spacetime(weekend_evening_full, hex_grid,
                               .loc_col = "index", .time_col = "TIME_PER_HOUR")

# Verify that resultant output is a spacetime cube
is_spacetime_cube(weekend_evening_cube)
```

The `TRUE` return confirms that `weekend_evening_cube` object is indeed an time-space cube.
:::

## Computation of Local G~i~\* statistics

To compute the local G~i~\* statistics, spatial weights must first be derived. The code chunks below will be used to identify neighbors and to derive an inverse distance weights for each target time period. `activate()` is used to first activate the geometry context, followed by `mutate()` to create two new columns `nb` and `wt`. Then activate the data context again and copy over the `nb` and `wt` columns to each time-slice using `set_nbs()` and `set_wts()`.

::: panel-tabset
### Spatial Weights for Weekday Morning

```{r}
weekday_morn_nb <- weekday_morn_cube %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

### Spatial Weights for Weekday Evening

```{r}
weekday_evening_nb <- weekday_evening_cube %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

### Spatial Weights for Weekend/PH Morning

```{r}
weekend_morn_nb <- weekend_morn_cube %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

### Spatial Weights for Weekend/PH Evening

```{r}
weekend_evening_nb <- weekend_evening_cube %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```
:::

In the code chunks below, the new columns are used to manually calculate the local G~i~\* for each location. This is done by grouping by `TIME_PER_HOUR` and using `local_gstar_perm()` of sfdep package. After which, `unnest()` is used to unnest the `gi_star` column of the newly created output.

::: panel-tabset
### G~i~\* for Weekday Morning

```{r}
#| eval: false
gi_weekday_morn <- weekday_morn_nb %>%
  group_by(TIME_PER_HOUR) %>%
  mutate(gi_star = local_gstar_perm(TRIPS, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

### G~i~\* for Weekday Evening

```{r}
#| eval: false
gi_weekday_evening <- weekday_evening_nb %>%
  group_by(TIME_PER_HOUR) %>%
  mutate(gi_star = local_gstar_perm(TRIPS, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

### G~i~\* for Weekend/PH Morning

```{r}
#| eval: false
gi_weekend_morn <- weekend_morn_nb %>%
  group_by(TIME_PER_HOUR) %>%
  mutate(gi_star = local_gstar_perm(TRIPS, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

### G~i~\* for Weekend/PH Evening

```{r}
#| eval: false
gi_weekend_evening <- weekend_evening_nb %>%
  group_by(TIME_PER_HOUR) %>%
  mutate(gi_star = local_gstar_perm(TRIPS, nb, wt)) %>%
  tidyr::unnest(gi_star)
```
:::

```{r}
#| echo: false
#| eval: false
# Save the output as rds files instead of rendering the above code chunks
write_rds(gi_weekday_morn, "data/rds/gi_weekday_morn.rds")
write_rds(gi_weekday_evening, "data/rds/gi_weekday_evening.rds")
write_rds(gi_weekend_morn, "data/rds/gi_weekend_morn.rds")
write_rds(gi_weekend_evening, "data/rds/gi_weekend_evening.rds")
```

```{r}
#| echo: false
# Save the output as rds files instead of rendering the above code chunks
gi_weekday_morn <- read_rds("data/rds/gi_weekday_morn.rds")
gi_weekday_evening <- read_rds("data/rds/gi_weekday_evening.rds")
gi_weekend_morn <- read_rds("data/rds/gi_weekend_morn.rds")
gi_weekend_evening <- read_rds("data/rds/gi_weekend_evening.rds")
```

## Mann-Kendall Test

With the above G~i~\* statistics, it is possible to evaluate each location for a trend using the Mann-Kendall test.

:::panel-tabset

### Mann-Kendall Test for Weekday Morning

The code chunk below performs the Mann-Kendall Test for hexagon grids 1251 and 2411. 

```{r}
mk_weekdaymorn <- gi_weekday_morn %>% 
  ungroup() %>% 
  filter(index == 1251 | index == 2411) |> 
  select(index, TIME_PER_HOUR, gi_star)

# Plot result using ggplot2 functions and ggplotly() of plotly package
p1 <- ggplot(data = mk_weekdaymorn, 
       aes(x = TIME_PER_HOUR, 
           y = gi_star,
           group = index,
           color = as.factor(index))) +
  geom_line() +
  theme_light()

ggplotly(p1)
```

```{r}
mk_weekdaymorn %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

> In the above result, sl is the p-value. 

::: callout-tip
## Insights

The chart and table above suggests a downward trend over the 4 hours of the morning peak hour crowd for these two hexagon grids, albeit an insignificant one. 
:::

### Mann-Kendall Test for Weekday Evening

The code chunk below performs the Mann-Kendall Test for hexagon grids 1251, 2411, 3239, 4349, and 4539.

```{r}
mk_weekdayevening <- gi_weekday_evening %>% 
  ungroup() %>% 
  filter(index %in% c(1251, 2411, 3239, 4349, 4539)) |> 
  select(index, TIME_PER_HOUR, gi_star)

# Plot result using ggplot2 functions and ggplotly() of plotly package
p2 <- ggplot(data = mk_weekdayevening, 
       aes(x = TIME_PER_HOUR, 
           y = gi_star,
           group = index,
           color = as.factor(index))) +
  geom_line() +
  theme_light()

ggplotly(p2)
```

```{r}
mk_weekdayevening %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

> In the above result, sl is the p-value. 

::: callout-tip
## Insights

The chart and table above suggests no significant trend difference over the 4 hours of the weekday evening peak hour crowd for the top hexagon grids. 
:::
:::

The Mann-Kendall test is replicated for each hexagon grid in the code chunk below. 

```{r}
#| eval: false
weekdaymorn_ehsa <- gi_weekday_morn %>%
  group_by(index) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)

# Save out the model
write_rds(weekdaymorn_ehsa, "data/rds/weekdaymorn_ehsa.rds")
```

```{r}
#| echo: false
# Load the model
weekdaymorn_ehsa <- read_rds("data/rds/weekdaymorn_ehsa.rds")
```

The output is then arranged to show significant emerging hot / cold spots. 

```{r}
weekdaymorn_emerging <- weekdaymorn_ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:10)

weekdaymorn_emerging
```

::: callout-tip
## Insights

At an alpha value of 0.05 (CI of 95%), the p-value > alpha value, suggesting there is insufficient evidence to reject the null hypothesis. 

At an alpha value of 0.1 (CI of 90%), the p-value < alpha value, suggesting there is sufficient evidence to reject the null hypothesis. 
:::

## Perform Emerging Hot Spot Analysis (EHSA)

In the code chunks below, `emerging_hotspot_analysis()` of **sfdep** package is used to perform Emerging Hot Spot Analysis on a spacetime object e.g. `weekday_morn_cube`. The `.var` argument takes the name of a variable of interest (e.g. `TRIPS`), the `k` argument is used to specify the number of time lags which is set to 1 by default, and the `nsim` argument maps the number of simulations to be performed. Lastly, ggplot2 functions are used to reveal the distribution of EHSA classes as a bar chart.

::: panel-tabset
### EHSA for Weekday Morning

```{r}
#| eval: false
wkd_morn_ehsa <- emerging_hotspot_analysis(
  x = gi_weekday_morn, 
  .var = "TRIPS", 
  k = 1, 
  nsim = 99
)

# Save out the model
write_rds(wkd_morn_ehsa, "data/rds/wkd_morn_ehsa.rds")
```

```{r}
#| echo: false
wkd_morn_ehsa <- read_rds("data/rds/wkd_morn_ehsa.rds")
```

```{r, fig.width=12}
# Visualise the distribution of classes
ggplot(data = wkd_morn_ehsa,
       aes(x = classification)) +
  geom_bar()
```

::: callout-tip
## Insights

More than half of the hexagon grids had no pattern detected - likely due to the large number of 0s in the data frame. The next largest class of hexagon grids are "sporadic cold spots", which refers to locations that are on-again then off-again cold spots.
:::

### EHSA for Weekday Evening

```{r}
#| eval: false
wkd_evening_ehsa <- emerging_hotspot_analysis(
  x = weekday_evening_cube, 
  .var = "TRIPS", 
  k = 1, 
  nsim = 99
)

# Save out the model
write_rds(wkd_evening_ehsa, "data/rds/wkd_evening_ehsa.rds")
```

```{r}
#| echo: false
wkd_evening_ehsa <- read_rds("data/rds/wkd_evening_ehsa.rds")
```

```{r, fig.width=12}
# Visualise the distribution of classes
ggplot(data = wkd_evening_ehsa,
       aes(x = classification)) +
  geom_bar()
```

::: callout-tip
## Insights

xxx
:::

### EHSA for Weekend/PH Morning

```{r}
#| eval: false
wke_morn_ehsa <- emerging_hotspot_analysis(
  x = weekend_morn_cube, 
  .var = "TRIPS", 
  k = 1, 
  nsim = 99
)

# Save out the model
write_rds(wke_morn_ehsa, "data/rds/wke_morn_ehsa.rds")
```

```{r}
#| echo: false
wke_morn_ehsa <- read_rds("data/rds/wke_morn_ehsa.rds")
```

```{r, fig.width=12}
# Visualise the distribution of classes
ggplot(data = wke_morn_ehsa,
       aes(x = classification)) +
  geom_bar()
```

::: callout-tip
## Insights

xxx
:::

### EHSA for Weekend/PH Evening

```{r}
#| eval: false
wke_evening_ehsa <- emerging_hotspot_analysis(
  x = weekend_evening_cube, 
  .var = "TRIPS", 
  k = 1, 
  nsim = 99
)

# Save out the model
write_rds(wke_evening_ehsa, "data/rds/wke_evening_ehsa.rds")
```

```{r}
#| echo: false
wke_evening_ehsa <- read_rds("data/rds/wke_evening_ehsa.rds")
```

```{r, fig.width=12}
# Visualise the distribution of classes
ggplot(data = wke_evening_ehsa,
       aes(x = classification)) +
  geom_bar()
```

::: callout-tip
## Insights

xxx
:::
:::

## Visualise EHSA

Lastly, visualise the geographic distribution of the EHSA classes. In the code chunks below, `hex_grid` sf data frame is joined to the EHSA data frames. Then, tmap functions will be used to plot categorical choropleth maps.

::: panel-tabset
### Choropleth Map for Weekday Morning

```{r}
wkd_evening_ehsa_grid <- hex_grid %>%
  left_join(wkd_evening_ehsa,
            by = c("index" = "location"))

# Plot the choropleth map for significant values
ehsa_sig <- wkd_evening_ehsa_grid  %>%
  filter(p_value < 0.1)

tmap_mode("view")

tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```
:::
